<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="shortcut icon" href="#"> <!-- 404 (Not Found) error failed to load favicon.ico  -->
    <link href="styles.css" rel="stylesheet"> <!--<link> is importing the custom css file-->
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Seeing It</title>
</head>
<body>
  <!-- Navbar Section -->
  <nav class="navbar">
    <div class="navbar__container">
      <a href="#home" class="navbar__logo">SEEING IT</a>
      <!-- When you minimize the screen -->
      <div class="navbar__toggle" id="mobile-menu">
        <span class="bar"></span> 
        <span class="bar"></span>
        <span class="bar"></span>
      </div>
      <!-- Without screen minimization -->
      <ul class="navbar__menu">
        <li class="navbar__item">
          <a href="#home" class="navbar__links" id="home-page">Home</a>
        </li>
        <li class="navbar__item">
          <a href="#services" class="navbar__links" id="services-page">Service</a>
        </li>
        <li class="navbar__item">
          <a href="#blog" class="navbar__links" id="blog-page">Blog</a>
        </li>
      </ul>
    </div>
  </nav>

  <!-- Home Section -->
  <div class="home__section" id="home">
    <div class="home__container">
      <h1 class="home__heading">Begin to Visualize</h1>
      <p class="home__description">Empowered by AI</p>
      <button class="main__btn"><a href="#services">Explore</a></button>
    </div>
  </div>

  <!-- Service Section -->
  <div class="services__section" id="services">
    <h1>Get Started</h1> <br>
    <p>1) Upload a file (png/jpg/jpeg) <br>
       2) Obtain the image's textual description <br>
       3) Translate it to audio! <br>
    </p> <br>
    <div class="services__wrapper">
      <div class="services__card">
        <h2 id="file__id">Click to Get File!</h2>
        <h2 id="file__id">ðŸ¡»</h2>
        <!--"Choose File" button that only accepts png and jpeg files-->
        <input class="choose__file" type="file" id="chooseFiles" accept="image/x-png,image/jpeg"
        name="image" onchange="showPreview(event)"/><br/>
      </div>
      <div class="services__card">
        <div class="image-preview" id="imagePreview">
          <div class="output__label" id="prev"><strong>Image Preview:</strong></div>
          <img src="" class="image__icon" id="image__preview" width="400px" height="350px">
        </div>
      </div>
      <div class="services__card">
        <form id="formData" enctype="multipart/form-data" onsubmit="return (returnText(event))">
          <h2>Click to Get Text!</h2>
          <h2>ðŸ¡»</h2>
          <!--"Submit" button-->
          <button class="desc__translate">
            <span>Description</span>
          </button>
          <div class="output__label2"><strong>Output Shown Below:</strong></div>
          <div id="output__area">
            <!-- <textarea id="output"></textarea> -->
          </div>
        </form>
      </div>
      <div class="services__card">
        <h2>Click to Convert!</h2>
        <h2>ðŸ¡»</h2>
        <form onsubmit="beginTranslation(event)">
          <button class="translate__button" id="startSynthesisAsyncButton">
            <i class="fas fa-volume-off spacing" aria-hidden="true"></i>Tap to Hear!
          </button>
        </form>
      </div>
    </div>
  </div>  

  <!-- Blog Section -->
  <div class="blog__section" id="blog">
    <div class="title__it"><u>How it was Made</u></div>
    <div class="blog__container">
      <h1 id="first">About Me</h1>
      <hr class="change__width"/>
      <h1 id="intro">My name is Derek Lam (he/him), and I am third year Computer Science student at the 
        University of California, Irvine. Outside of school, I like to play tennis, go on hikes, work out, 
        and learn about new technologies. I hope to build technical solutions that can impact diverse 
        audiences. </h1>
      <h1 id="first">The Premise</h1>
      <hr class="change__width"/>
      <h1 id="intro">While everyone is different in many ways, that should not stop 
        certain types of people from living a normal life.
        
        My project, Seeing It, aims to target individuals with visual
        impairment. This tool allows the blind to "see" the world through
        just a click of a button.
        
        Seeing It works by first capturing a photo, then analyzes what's going
        on in the image to obtain a textual output, and finally reads it
        to the user through a built-in speech algorithm.</h1>
      <h1 id="first">Project Structure</h1>
      <hr class="change__width"/>
      <img class="flowchart" src="https://lh3.googleusercontent.com/x8y3-ItRuRR-Pe_3FT8hy4TUOa0k_oDmuIIRlXXbKQhLkWZnJ5wBKA9al4UMxllWc_Gr2DdK7oEqaxFJ3W_0CFb3P9ApgxM56rkaI0EUee4Z4-TB0I76F7CzKxx-RGlU1JQJk2XLBw=w2400"/>
      <h1 id="first">Tools and Technologies</h1>
      <hr class="change__width"/>
      <h1 id="intro">Seeing It is built upon 3 main components: <br><br>
        1) Uploading a file <br>
        <img class="first__upload" src="https://lh3.googleusercontent.com/KgzAXXDKhKfghPOj2-FXCDCyQFG1aWa-TlUIz2HWD8TV63miVg7r1S5ce0B2NxB24nHQb7KA0swGMbyV3enUeV2ieDLR7X_--3xxPynAz8WSLC955IktIOF9c7vZ3dCN3W543AF2Ew=w2400"/> <br>
        In order to upload an image to the UI, we must have an <bold>input</bold>
        that accepts a file and a <bold>form</bold> that will hold the data to later
        be processed. From there, we get the id's from both of these html
        elements and create a variable that will store a <bold>File</bold> object 
        containing the file name. After that, a <bold>FormData</bold> object is
        made since we're uploading an image as the body of the request
        we're going to make. Finally, take the File object and append it
        to the FormData object as a key-value pair (the key can be named 
        anything). <br><br><br><br>
        
        2) Image Analyzer API <br><br>
        Since we want a textual description about an image, we will be using
        the Computer Vision service. The Image Analyzer API is in the form
        of an HTTP Trigger, where the backend code is covered since we are using
        Microsoft's Azure Serverless Functions. 
        <img class="second__upload" src="https://lh3.googleusercontent.com/8h_8OTdQjDM1sKIUxGhvmbMj2OUO2M8LAt7rYrhtpvYrF5fzSjQUNZN7YbzdQ5na5NM5DqUdMD_a-9UGjAFpborwZcbF70aaCQvhGVvcPeXaNnPE0CiHtzzzfa7_Q_R9rhhNenSRNA=w2400"/> <br>

        To begin, install and import
        <bold>node-fetch</bold> (used to make requests to API) and <bold>parse-multipart</bold> 
        (parses raw data for simplified usage). It is crucial to also include
        the <bold>subscription key</bold> and <bold>endpoint</bold> (from the Computer Vision resource)
        because we will need them to make the API call. <br><br>

        Next, set the header to <bold>content-type</bold> because we're using form-data
        to make a <bold>POST</bold> request (don't forget to set the body!). Using the 
        "body" and "boundary" variables, we use multipart to parse the data and obtain
        two lists of information: the file name and type of file. Since we want
        the file name, we access the first index of "parts" and get the <bold>data</bold>
        attribute. Now that we have obtained the image, it will be used as a parameter 
        for the function that calls the Computer Vison API (I will explain how the method
        is made shortly). Finally, the image's description and confidence
        (how certain the algorithm is in being correct, usually in the form of a percentage)
        are both outputted in the form of text.

        <img class="third__upload" src="https://lh3.googleusercontent.com/h9Yo3XuF8y4lpl2uiRUzWPLtZp09J07YCxhGH5M-go3XsOhgEA4mpdd7CkLp098xuMM-nY97MkAlQUaaNXyZ-qT49opsKmPi7i2ze0HVD9ZdfkJVv9xjNPH5vkEA-cNV0nns6InPyw=w2400"/><br>
      
        Lets talk about how the function is made. Since we're making a call to the 
        Computer Vision API and looking specifically for the image analysis feature,
        we create a variable that holds both the endpoint and added extension. The parameters
        are then established to look for what information we need from the API. Following
        that, a POST request is made with the image and key being utilized. Finally, we
        return the information as a json object and receive the data containing the 
        <bold>description</bold> attribute. <br><br><br><br>

        2.5) Steps 1 + 2
        <img class="fourth__upload" src="https://lh3.googleusercontent.com/hjfzgZyeSHZfpOuD2aSl0KTW5r85OEDnQTVuNgprDGBVLjc8rYK99qbcLyrQ-dNuyGzIxg6rYQXkqf1V5smomCsLD6md5sSPBYqoabKmxSQ9ZU6cygwLprjyrFRnVMjnyzRoUzjMDg=w2400"/><br>
        With our files being able to properly upload and our Image Analyzer API working,
        its time to put it all together. In order to made a request from a web page 
        to an API, we must use the XMLHttpRequest object. After creating it, we use
        the <bold>XMLHttpRequest.open()</bold> method to make a POST request to the Image Analyzer API and 
        <bold>XMLHttpRequest.send()</bold> to attach the file as the request body. Finally, the <bold>XMLHttpRequest.onload()</bold>
        method is called to get our response, where JQuery is used. 
        If everything ends up running smoothly, output the desired response, but if not, 
        show that an error occurred.<br><br><br><br>

        3) Text-to-Speech API
        <img class="fifth__upload" src="https://lh3.googleusercontent.com/KWM9wgaqTUo62vKYBeAFE-CaD1QrBLdTk7Ld_inrACU_91UwkKfo4a8JsTUo3uTGkyo9fROklzTESINYwIXwhNIobpRwkOzpjms3HMZhp7qPJnJpPaa0WzRdbfeWYiWLWgqZxVPEGw=w2400"/><br>
        Since we're using a button to activate the Text-to-Speech API,
        we have to make a reference to it somehow, so in this case, by
        the element's id. The fifth line is where we are setting the innerhtml
        to "volumeOn," which is for styling purposes. Next, we need to store the output
        description as a variable since we need to translate it to speech. We do
        so by referencing the id labeled <bold>output</bold> (remember that "#output" was what
        we used to get the image description when using XMLHttpRequest.onload()) and access its <bold>textContent</bold> attribute.
        Finally, make it so that you can't click the button again while the translation
        is active.<br>

        <img class="sixth__upload" src="https://lh3.googleusercontent.com/v4cCTx82qo1rwQZYo_Fe3eF4X_G7IEGExUZEs5PX1NqAQRqJ6rWeX71EiC4eJN2p-HRaV54z6sMgtsDFfcvOEP0BtfukxZfnFzM7gaeNinCs0uHdoP6vAygrHEJUoieziPBOU4ftMQ=w2400"/><br>
        We will be using Microsoft's Speech SDK to construct our API. First, create 
        a <bold>SpeakerAudioDestination</bold> object to establish where the audio will be played.
        Then use <bold>SpeakerAudioDestination().onAudioEnd()</bold> 
        (the code above is formatted differently, but works the same way) to add events while the speech is active. 
        In this case, once the audio has completed, make the button clickable and have
        the "volumeOff" icon displayed. <br><br>
      
        After completing those steps, we have to get our 
        <bold>credentials</bold> (e.g. Speech resource's <bold>subscription key</bold> and <bold>location</bold>) and
        indicate where the speech should be produced. From there, make a <bold>SpeechSynthesizer</bold>
        object that takes the credentials and speaker output as parameters. Finally, 
        use the <bold>SpeechSynthesizer().speakTextAsync()</bold> method to activate the speech, where the parameters
        are the text itself and functions that can display the result and error.<br>

      <h1 id="first">Challenges + Lessons Learned</h1>
      <hr class="change__width"/>
      <h1 id="intro">The main challenges I faced when building my software were calling the 
        Image Analyzer API to output the result to my UI and getting the Text-to-Speech
        API to emit audio.<br><br>
      
        I ran into this problem of calling the 
        Image Analyzer API to produce output on my website 
        because I thought the idea of using "fetch" to call
        APIs in Http Trigger functions would apply to my project. This was because
        working with Http Triggers were the only reference experience I had when it
        came to dealing with APIs during my time at the Serverless Camp. I soon
        realized that there had to be a different way of executing this problem
        since I was dealing with a plain script file. However, after doing some 
        research, I found that making XMLHttpRequests were a common way of 
        listening to events during requests made to servers. Through implementing
        the ".open()," ".send()," and ".onload()" methods from the XMLHttpRequest
        object, I was able to successfully analyze a given image and output its
        textual description.<br><br>

        Additionally, the audio did not emit from the Text-to-Speech API because
        following the Microsoft documentation on how to translate text to speech,
        I found that using the ".fromDefaultSpeakerOutput()" method to configure
        the audio would be the way to do so. However, it ended up not working because
        this function is not supported on browsers, according to several forums. In the
        end, I asked my mentor Sanket about this issue and he helped me reach a solution.<br>
      </h1>
      <h1 id="first">Moving Forward</h1>
      <hr class="change__width"/>
      <h1 id="intro">I hope to take my project and implement it in hardware so that it can be
        utilized in any setting. Check out my github repository to view the code
        that made up my website!<br>
      </h1>
      <h1 id="first">Thanks and Acknowledgements</h1>
      <hr class="change__width"/>
      <h1 id="intro">I would like to thank Bit Project for giving me the opportunity to
        join the Serverless Camp and learn about cloud computing with Microsoft! Everyone
        in the camp was very supportive of one another and it felt great to learn
        in an environment where people came from various experience levels.<br><br>

        I want to give a huge token of appreciation to my mentor Sanket! He has supported me so much
        throughout the camp and helped pique my interest in working with Azure and
        web development.<br><br>

        Overall, I am thankful to have gone through this experience as it was a great
        way to learn new skills while being surrounded by like-minded individuals.
      </h1>
    </div>
  </div>
</body> 

<!--IMPORTS-->
<!--JQuery-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
<!--script.js-->
<script src="script.js" type="text/javascript"></script>
<!--forStyles.js-->
<script src="forStyles.js"></script>
<!--SpeechSDK Class reference-->
<script src="https://aka.ms/csspeech/jsbrowserpackageraw"></script>
<!--Volume Import-->
<script src="https://kit.fontawesome.com/7fb8d134c9.js" crossorigin="anonymous"></script>
